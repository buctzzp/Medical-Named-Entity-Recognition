#!/usr/bin/env python
# evaluate.py â€” æ¨¡å‹è¯„ä¼°è„šæœ¬

import os
import torch
import argparse
import json
import logging
from tqdm import tqdm
from datetime import datetime
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
from transformers import BertTokenizerFast
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

from model.bert_crf_model import BertCRF
from model.bert_attention_crf_model import BertAttentionCRF
from model.model_factory import ModelFactory
from utils import NERDataset, get_label_list
from config import model_config

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='ä¸­æ–‡åŒ»ç–—NERæ¨¡å‹è¯„ä¼°å·¥å…·')
parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹æƒé‡è·¯å¾„')
parser.add_argument('--pretrained_model', type=str, default=model_config.get('pretrained_model_name', 'bert-base-chinese'), 
                   help='é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´')
parser.add_argument('--test_file', type=str, default=model_config.get('test_path', 'data/test.txt'), help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
parser.add_argument('--batch_size', type=int, default=model_config.get('eval_batch_size', 32), help='è¯„ä¼°æ‰¹æ¬¡å¤§å°')
parser.add_argument('--max_length', type=int, default=model_config.get('max_len', 128), help='æœ€å¤§åºåˆ—é•¿åº¦')
parser.add_argument('--use_attention', action='store_true', default=model_config.get('use_attention', False), 
                   help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æ¨¡å‹')
parser.add_argument('--use_bilstm', action='store_true', default=model_config.get('use_bilstm', False), 
                   help='æ˜¯å¦ä½¿ç”¨BiLSTMå±‚')
parser.add_argument('--no_bilstm', action='store_true', help='ç¦ç”¨BiLSTMå±‚ï¼ˆè¦†ç›–é»˜è®¤é…ç½®ï¼‰')
parser.add_argument('--output_dir', type=str, default='results', help='è¾“å‡ºç›®å½•')
parser.add_argument('--detailed_report', action='store_true', help='è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š')
parser.add_argument('--confusion_matrix', action='store_true', help='ç”Ÿæˆæ··æ·†çŸ©é˜µ')
parser.add_argument('--save_predictions', action='store_true', help='ä¿å­˜é¢„æµ‹ç»“æœ')
args = parser.parse_args()

# no_bilstm æ¯” use_bilstm ä¼˜å…ˆçº§é«˜
use_bilstm = args.use_bilstm
if args.no_bilstm:
    use_bilstm = False

# æ£€æµ‹è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ç”Ÿæˆæ¨¡å‹IDå’Œç­¾åï¼Œä¸train_enhanced.pyå’Œpredict_enhanced.pyä¿æŒä¸€è‡´
model_id = args.pretrained_model.replace('-', '_')
model_type = "attention" if args.use_attention else "base"
bilstm_status = "with_bilstm" if use_bilstm else "no_bilstm"
model_signature = f"{model_id}_{model_type}_{bilstm_status}"

# è®¾ç½®è¾“å‡ºç›®å½•
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
eval_dir = os.path.join(args.output_dir, model_id, f"eval_{model_signature}_{timestamp}")
os.makedirs(eval_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—
log_file = os.path.join(eval_dir, f'evaluation.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # æŒ‡å®šç¼–ç ä¸ºutf-8
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®°å½•è¯„ä¼°é…ç½® (ç§»é™¤emoji)
logger.info("="*50)
logger.info("åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯„ä¼°å¼€å§‹")
logger.info("="*50)
logger.info(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
logger.info(f"æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
logger.info(f"BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
logger.info(f"æ¨¡å‹ç­¾å: {model_signature}")
logger.info(f"æ¨¡å‹è·¯å¾„: {args.model}")
logger.info(f"æµ‹è¯•æ–‡ä»¶: {args.test_file}")
logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
logger.info(f"æœ€å¤§é•¿åº¦: {args.max_length}")
logger.info("="*50)

# æ§åˆ¶å°è¾“å‡ºå¯ä»¥ä¿ç•™emoji
print(f"ğŸš€ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
print(f"ğŸ” æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
print(f"ğŸ§  BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
print(f"ğŸ“Š æ¨¡å‹ç­¾å: {model_signature}")
print(f"ğŸ“ è¯„ä¼°ç»“æœå°†ä¿å­˜è‡³: {eval_dir}")

# åŠ è½½æ ‡ç­¾åˆ—è¡¨
train_paths = []
train_path = model_config.get('train_path', 'data/train.txt')
if os.path.exists(train_path):
    train_paths.append(train_path)

dev_path = model_config.get('dev_path', 'data/dev.txt')
if os.path.exists(dev_path):
    train_paths.append(dev_path)

# å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨æµ‹è¯•æ–‡ä»¶æ¥è·å–æ ‡ç­¾
if not train_paths and os.path.exists(args.test_file):
    train_paths.append(args.test_file)

if not train_paths:
    logger.error("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶æ¥è·å–æ ‡ç­¾åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
    print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶æ¥è·å–æ ‡ç­¾åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
    exit(1)

label_list = get_label_list(train_paths)

# æ„å»ºæ ‡ç­¾æ˜ å°„
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}
num_labels = len(label_list)

logger.info(f"æ ‡ç­¾æ€»æ•°: {num_labels}")
logger.info(f"æ ‡ç­¾åˆ—è¡¨: {label_list}")

# ä½¿ç”¨æ¨¡å‹å·¥å‚è·å–ä¸æ¨¡å‹åŒ¹é…çš„tokenizer
tokenizer = ModelFactory.get_tokenizer_for_model(args.pretrained_model)

# åŠ è½½æ¨¡å‹ - ä½¿ç”¨ModelFactoryæ­£ç¡®åˆ›å»ºæ¨¡å‹
model_type_str = 'bert_attention_crf' if args.use_attention else 'bert_crf'
model = ModelFactory.create_model(
    model_type=model_type_str,
    num_labels=num_labels,
    pretrained_model_name=args.pretrained_model
)

# è®¾ç½®æ¨¡å‹çš„BiLSTMå‚æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
if hasattr(model, 'use_bilstm'):
    model.use_bilstm = use_bilstm
    # æ›´æ–°BiLSTMç›¸å…³é…ç½®
    if hasattr(model, 'lstm_hidden_size'):
        model.lstm_hidden_size = model_config.get('lstm_hidden_size', 128)
    if hasattr(model, 'lstm_layers'):
        model.lstm_layers = model_config.get('lstm_layers', 1)
    if hasattr(model, 'lstm_dropout'):
        model.lstm_dropout = model_config.get('lstm_dropout', 0.1)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
try:
    model.load_state_dict(torch.load(args.model, map_location=device))
    logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model}")
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model}")
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ä½¿ç”¨äº†ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹")
    exit(1)

model.to(device)
model.eval()

# åŠ è½½æµ‹è¯•æ•°æ®
if not os.path.exists(args.test_file):
    logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {args.test_file}")
    print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {args.test_file}")
    exit(1)

test_dataset = NERDataset(args.test_file, tokenizer, label2id, args.max_length)
test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                         num_workers=4 if torch.cuda.is_available() else 0)

logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")
print(f"ğŸ“„ æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")

# è¯„ä¼°å‡½æ•°
def evaluate():
    model.eval()
    true_labels = []
    pred_labels = []
    all_sentences = []
    
    # é¢„æµ‹ç»“æœ
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
            batch = {k: v.to(device) for k, v in batch.items()}
            
            # åŸå§‹æ–‡æœ¬ï¼ˆå¯¹äºä¿å­˜é¢„æµ‹ç»“æœï¼‰
            tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in batch['input_ids']]
            
            # æ¨¡å‹é¢„æµ‹
            pred = model(batch['input_ids'], batch['attention_mask'], 
                         batch.get('token_type_ids', None))
            
            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            for i in range(batch['input_ids'].size(0)):
                label_ids = batch['labels'][i].tolist()
                # æ ¹æ®æ¨¡å‹è¾“å‡ºç±»å‹è·å–é¢„æµ‹çš„æ ‡ç­¾ID
                if isinstance(pred, list):
                    pred_ids = pred[i]
                else:
                    pred_ids = pred[i].argmax(dim=-1).tolist()
                
                sample_tokens = tokens[i]
                word_labels = []
                word_preds = []
                words = []
                
                for token, true_id, pred_id in zip(sample_tokens, label_ids, pred_ids):
                    if token in ["[CLS]", "[SEP]", "[PAD]"] or token.startswith("<") or true_id == -100:
                        continue
                    
                    # ä¿å­˜å®é™…çš„token
                    words.append(token.replace("##", ""))
                    word_labels.append(id2label[true_id])
                    word_preds.append(id2label[pred_id])
                
                if word_labels:
                    true_labels.append(word_labels)
                    pred_labels.append(word_preds)
                    all_sentences.append(words)
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    precision = precision_score(true_labels, pred_labels)
    recall = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)
    
    logger.info(f"æ•´ä½“æ€§èƒ½: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
    print(f"æ•´ä½“æ€§èƒ½: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    report = classification_report(true_labels, pred_labels, digits=4, output_dict=True)
    
    # æ‰“å°æ¯ç§å®ä½“ç±»å‹çš„è¯¦ç»†æŒ‡æ ‡
    entity_metrics = {}
    print("\nå®ä½“ç±»å‹æ€§èƒ½æŒ‡æ ‡:")
    for entity_type, metrics in report.items():
        if entity_type != "micro avg" and entity_type != "macro avg" and entity_type != "weighted avg" and isinstance(metrics, dict):
            entity_p = metrics['precision']
            entity_r = metrics['recall']
            entity_f1 = metrics['f1-score']
            support = metrics['support']
            entity_metrics[entity_type] = {
                'precision': entity_p,
                'recall': entity_r,
                'f1': entity_f1,
                'support': support
            }
            print(f"  {entity_type}: P={entity_p:.4f}, R={entity_r:.4f}, F1={entity_f1:.4f}, æ ·æœ¬æ•°={support}")
            logger.info(f"å®ä½“ç±»å‹ {entity_type}: P={entity_p:.4f}, R={entity_r:.4f}, F1={entity_f1:.4f}, æ ·æœ¬æ•°={support}")
    
    # è®¡ç®—æ¯ä¸ªéOæ ‡ç­¾çš„å®ä½“ç±»å‹
    entity_types = []
    for label in label_list:
        if label.startswith("B-") or label.startswith("I-"):
            entity_type = label[2:]  # å»æ‰"B-"æˆ–"I-"å‰ç¼€
            if entity_type not in entity_types:
                entity_types.append(entity_type)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results = {
        'model_info': {
            'pretrained_model': args.pretrained_model,
            'model_signature': model_signature,
            'model_path': args.model,
            'use_attention': args.use_attention,
            'use_bilstm': use_bilstm
        },
        'eval_info': {
            'test_file': args.test_file,
            'num_samples': len(test_dataset),
            'timestamp': timestamp
        },
        'metrics': {
            'overall': {
                'precision': precision,
                'recall': recall,
                'f1': f1
            },
            'entity_metrics': entity_metrics
        }
    }
    
    # ä¿å­˜JSONç»“æœ
    results_file = os.path.join(eval_dir, 'evaluation_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    # å¦‚æœè¦ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if args.detailed_report:
        detailed_report_file = os.path.join(eval_dir, 'detailed_classification_report.txt')
        with open(detailed_report_file, 'w', encoding='utf-8') as f:
            f.write(classification_report(true_labels, pred_labels, digits=4))
    
    # å¦‚æœè¦ä¿å­˜é¢„æµ‹ç»“æœ
    if args.save_predictions:
        predictions = []
        for i, (words, true_ls, pred_ls) in enumerate(zip(all_sentences, true_labels, pred_labels)):
            predictions.append({
                'id': i,
                'words': words,
                'true_labels': true_ls,
                'pred_labels': pred_ls
            })
        
        pred_file = os.path.join(eval_dir, 'predictions.json')
        with open(pred_file, 'w', encoding='utf-8') as f:
            json.dump(predictions, f, ensure_ascii=False, indent=4)
    
    # ç»˜åˆ¶æ€§èƒ½æ¡å½¢å›¾
    plt.figure(figsize=(12, 6))
    metrics_df = pd.DataFrame([
        {'Entity Type': 'Overall', 'Precision': precision, 'Recall': recall, 'F1': f1}
    ])
    
    for entity_type, metrics in entity_metrics.items():
        metrics_df = pd.concat([metrics_df, pd.DataFrame([{
            'Entity Type': entity_type,
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1': metrics['f1']
        }])], ignore_index=True)
    
    metrics_melted = pd.melt(metrics_df, id_vars=['Entity Type'], 
                            value_vars=['Precision', 'Recall', 'F1'],
                            var_name='Metric', value_name='Score')
    
    sns.barplot(data=metrics_melted, x='Entity Type', y='Score', hue='Metric')
    plt.title('NER è¯„ä¼°æ€§èƒ½æŒ‡æ ‡')
    plt.xlabel('å®ä½“ç±»å‹')
    plt.ylabel('å¾—åˆ†')
    plt.ylim(0, 1.0)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(eval_dir, 'entity_performance.png'))
    
    # å¦‚æœéœ€è¦æ··æ·†çŸ©é˜µ
    if args.confusion_matrix and entity_types:
        # å‡†å¤‡æ··æ·†çŸ©é˜µæ•°æ®
        all_true_flat = []
        all_pred_flat = []
        
        for true_ls, pred_ls in zip(true_labels, pred_labels):
            # æå–å®ä½“ç±»å‹ï¼ˆå»æ‰BIOå‰ç¼€ï¼‰
            true_entity_types = ['O' if label == 'O' else label[2:] for label in true_ls]
            pred_entity_types = ['O' if label == 'O' else label[2:] for label in pred_ls]
            
            all_true_flat.extend(true_entity_types)
            all_pred_flat.extend(pred_entity_types)
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„å®ä½“ç±»å‹ï¼ˆåŒ…æ‹¬Oï¼‰
        unique_entity_types = ['O'] + entity_types
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        confusion = np.zeros((len(unique_entity_types), len(unique_entity_types)))
        entity_to_idx = {entity: idx for idx, entity in enumerate(unique_entity_types)}
        
        for true_type, pred_type in zip(all_true_flat, all_pred_flat):
            confusion[entity_to_idx[true_type]][entity_to_idx[pred_type]] += 1
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(10, 8))
        sns.heatmap(confusion, annot=True, fmt='g', cmap='Blues',
                   xticklabels=unique_entity_types, yticklabels=unique_entity_types)
        plt.xlabel('é¢„æµ‹ç±»å‹')
        plt.ylabel('çœŸå®ç±»å‹')
        plt.title('å®ä½“ç±»å‹æ··æ·†çŸ©é˜µ')
        plt.tight_layout()
        plt.savefig(os.path.join(eval_dir, 'confusion_matrix.png'))
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³ {eval_dir}")
    logger.info(f"è¯„ä¼°å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³ {eval_dir}")
    
    return precision, recall, f1, report

if __name__ == "__main__":
    evaluate()