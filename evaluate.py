#!/usr/bin/env python
# evaluate.py â€” æ¨¡å‹è¯„ä¼°è„šæœ¬

import os
import torch
import argparse
import json
import logging
import sys  # æ·»åŠ sysæ¨¡å—
from tqdm import tqdm
from datetime import datetime
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
from transformers import BertTokenizerFast
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib as mpl

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
# å°è¯•æ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“
try:
    from matplotlib.font_manager import FontProperties
    # Windowså¸¸è§ä¸­æ–‡å­—ä½“
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'FangSong', 'KaiTi', 
                    'STKaiti', 'STSong', 'STFangsong', 'STXihei', 'STZhongsong']
    font_found = False
    for font in chinese_fonts:
        try:
            FontProperties(fname=font)
            plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']
            font_found = True
            print(f"ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
            break
        except:
            continue
    
    if not font_found:
        print("è­¦å‘Š: æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸æ­£ç¡®")
except:
    print("è­¦å‘Š: é…ç½®ä¸­æ–‡å­—ä½“å¤±è´¥ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸æ­£ç¡®")

from model.bert_crf_model import BertCRF
from model.bert_attention_crf_model import BertAttentionCRF
from model.model_factory import ModelFactory
from utils import NERDataset, get_label_list
from config import model_config

# æ£€æŸ¥é˜²æ­¢æ¨¡å—è¢«å¤šæ¬¡å¯¼å…¥æ‰§è¡Œ
# å¦‚æœè„šæœ¬ç›´æ¥è¿è¡Œè€Œä¸æ˜¯è¢«å¯¼å…¥ï¼Œ__name__ä¼šæ˜¯'__main__'
if __name__ == "__main__":
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ä¸­æ–‡åŒ»ç–—NERæ¨¡å‹è¯„ä¼°å·¥å…·')
    parser.add_argument('--model', type=str, default=model_config.get('best_model_path', 'models/best_model.pth'), 
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--pretrained_model', type=str, default=model_config.get('bert_model_name', 'bert-base-chinese'), 
                       help='é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´')
    parser.add_argument('--test_file', type=str, default=model_config.get('test_path', 'data/test.txt'), help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=model_config.get('eval_batch_size', 32), help='è¯„ä¼°æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_length', type=int, default=model_config.get('max_len', 128), help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--use_attention', action='store_true', default=model_config.get('use_attention', False), 
                       help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æ¨¡å‹')
    parser.add_argument('--use_bilstm', action='store_true', default=model_config.get('use_bilstm', False), 
                       help='æ˜¯å¦ä½¿ç”¨BiLSTMå±‚')
    parser.add_argument('--no_bilstm', action='store_true', help='ç¦ç”¨BiLSTMå±‚ï¼ˆè¦†ç›–é»˜è®¤é…ç½®ï¼‰')
    parser.add_argument('--output_dir', type=str, default='results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--detailed_report', action='store_true', help='è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š')
    parser.add_argument('--confusion_matrix', action='store_true', help='ç”Ÿæˆæ··æ·†çŸ©é˜µ')
    parser.add_argument('--save_predictions', action='store_true', help='ä¿å­˜é¢„æµ‹ç»“æœ')
    args = parser.parse_args()

    # no_bilstm æ¯” use_bilstm ä¼˜å…ˆçº§é«˜
    use_bilstm = args.use_bilstm
    if args.no_bilstm:
        use_bilstm = False

    # æ£€æµ‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ç”Ÿæˆæ¨¡å‹IDå’Œç­¾åï¼Œä¸train_enhanced.pyå’Œpredict_enhanced.pyä¿æŒä¸€è‡´
    model_id = args.pretrained_model.replace('-', '_').replace('/', '_')
    model_type = "attention" if args.use_attention else "base"
    bilstm_status = "with_bilstm" if use_bilstm else "no_bilstm"
    model_signature = f"{model_id}_{model_type}_{bilstm_status}"

    # æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹è·¯å¾„
    # ä½¿ç”¨å®‰å…¨è·¯å¾„ï¼Œé¿å…æ–œæ é—®é¢˜
    safe_model_id = model_id.replace('/', '_')
    model_dir = os.path.join(model_config.get('model_dir', 'models'), safe_model_id)
    os.makedirs(model_dir, exist_ok=True)

    if args.model == 'best':
        model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
            if not os.path.exists(model_path):
                model_path = model_config.get('best_model_path', 'models/best_model.pth')  # å›é€€åˆ°é»˜è®¤è·¯å¾„
    elif args.model == 'final':
        model_path = os.path.join(model_dir, f"final_model_{model_signature}.pth")
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, f"final_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
            if not os.path.exists(model_path):
                model_path = model_config.get('final_model_path', 'models/final_model.pth')  # å›é€€åˆ°é»˜è®¤è·¯å¾„
    elif args.model == 'quantized':
        model_path = os.path.join(model_dir, f"quantized_model_{model_signature}.pth")
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, f"quantized_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
            if not os.path.exists(model_path):
                model_path = os.path.join(model_dir, "quantized_model.pth")  # å›é€€åˆ°é»˜è®¤è·¯å¾„
    elif args.model == 'pruned':
        model_path = os.path.join(model_dir, f"pruned_model_{model_signature}.pth")
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, f"pruned_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
            if not os.path.exists(model_path):
                logger.warning(f"è­¦å‘Š: å‰ªææ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹ä»£æ›¿")
                model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
                if not os.path.exists(model_path):
                    model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
                    if not os.path.exists(model_path):
                        model_path = model_config.get('best_model_path', 'models/best_model.pth')  # å†æ¬¡å›é€€
    else:
        # ç”¨æˆ·æŒ‡å®šçš„å…·ä½“è·¯å¾„
        model_path = args.model

    # è®¾ç½®è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # ä½¿ç”¨å®‰å…¨è·¯å¾„ï¼Œé¿å…æ–œæ é—®é¢˜
    safe_model_id = model_id.replace('/', '_')
    eval_dir = os.path.join(args.output_dir, safe_model_id, f"eval_{model_signature}_{timestamp}")
    os.makedirs(eval_dir, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(eval_dir, f'evaluation.log')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),  # æŒ‡å®šç¼–ç ä¸ºutf-8
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    # è®°å½•è¯„ä¼°é…ç½® (ç§»é™¤emoji)
    logger.info("="*50)
    logger.info("åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æ¨¡å‹è¯„ä¼°å¼€å§‹")
    logger.info("="*50)
    logger.info(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
    logger.info(f"æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
    logger.info(f"BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
    logger.info(f"æ¨¡å‹ç­¾å: {model_signature}")
    logger.info(f"æ¨¡å‹è·¯å¾„: {model_path}")
    logger.info(f"æµ‹è¯•æ–‡ä»¶: {args.test_file}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"æœ€å¤§é•¿åº¦: {args.max_length}")
    logger.info("="*50)

    # æ§åˆ¶å°è¾“å‡ºå¯ä»¥ä¿ç•™emoji
    print(f"ğŸš€ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
    print(f"ğŸ” æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
    print(f"ğŸ§  BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
    print(f"ğŸ“Š æ¨¡å‹ç­¾å: {model_signature}")
    print(f"ğŸ“ è¯„ä¼°ç»“æœå°†ä¿å­˜è‡³: {eval_dir}")

    # åŠ è½½æ ‡ç­¾åˆ—è¡¨
    train_paths = []
    train_path = model_config.get('train_path', 'data/train.txt')
    if os.path.exists(train_path):
        train_paths.append(train_path)

    dev_path = model_config.get('dev_path', 'data/dev.txt')
    if os.path.exists(dev_path):
        train_paths.append(dev_path)

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨æµ‹è¯•æ–‡ä»¶æ¥è·å–æ ‡ç­¾
    if not train_paths and os.path.exists(args.test_file):
        train_paths.append(args.test_file)

    if not train_paths:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶æ¥è·å–æ ‡ç­¾åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶æ¥è·å–æ ‡ç­¾åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
        sys.exit(1)  # ä½¿ç”¨sys.exitæ›¿ä»£exit

    label_list = get_label_list(train_paths)

    # æ„å»ºæ ‡ç­¾æ˜ å°„
    label2id = {label: i for i, label in enumerate(label_list)}
    id2label = {i: label for label, i in label2id.items()}
    num_labels = len(label_list)

    logger.info(f"æ ‡ç­¾æ€»æ•°: {num_labels}")
    logger.info(f"æ ‡ç­¾åˆ—è¡¨: {label_list}")

    # ä½¿ç”¨æ¨¡å‹å·¥å‚è·å–ä¸æ¨¡å‹åŒ¹é…çš„tokenizer
    tokenizer = ModelFactory.get_tokenizer_for_model(args.pretrained_model)

    # åŠ è½½æ¨¡å‹ - ä½¿ç”¨ModelFactoryæ­£ç¡®åˆ›å»ºæ¨¡å‹
    model_type_str = 'bert_attention_crf' if args.use_attention else 'bert_crf'
    model = ModelFactory.create_model(
        model_type=model_type_str,
        num_labels=num_labels,
        pretrained_model_name=args.pretrained_model
    )

    # è®¾ç½®æ¨¡å‹çš„BiLSTMå‚æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    if hasattr(model, 'use_bilstm'):
        model.use_bilstm = use_bilstm
        # æ›´æ–°BiLSTMç›¸å…³é…ç½®
        if hasattr(model, 'lstm_hidden_size'):
            model.lstm_hidden_size = model_config.get('lstm_hidden_size', 128)
        if hasattr(model, 'lstm_layers'):
            model.lstm_layers = model_config.get('lstm_layers', 1)
        if hasattr(model, 'lstm_dropout'):
            model.lstm_dropout = model_config.get('lstm_dropout', 0.1)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    try:
        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ä½¿ç”¨äº†ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¨¡å‹æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹")
        sys.exit(1)  # ä½¿ç”¨sys.exitæ›¿ä»£exit

    model.to(device)
    model.eval()

    # åŠ è½½æµ‹è¯•æ•°æ®
    if not os.path.exists(args.test_file):
        logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {args.test_file}")
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {args.test_file}")
        sys.exit(1)  # ä½¿ç”¨sys.exitæ›¿ä»£exit

    test_dataset = NERDataset(args.test_file, tokenizer, label2id, args.max_length)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                             num_workers=4 if torch.cuda.is_available() else 0)

    logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")
    print(f"ğŸ“„ æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")

    # è¾…åŠ©å‡½æ•°ï¼šç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿç±»å‹
    def convert_to_native_python_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return convert_to_native_python_types(obj.tolist())
        elif isinstance(obj, dict):
            return {key: convert_to_native_python_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native_python_types(item) for item in obj]
        else:
            return obj

    # è¯„ä¼°å‡½æ•°
    def evaluate():
        model.eval()
        true_labels = []
        pred_labels = []
        all_sentences = []
        
        # é¢„æµ‹ç»“æœ
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
                batch = {k: v.to(device) for k, v in batch.items()}
                
                # åŸå§‹æ–‡æœ¬ï¼ˆå¯¹äºä¿å­˜é¢„æµ‹ç»“æœï¼‰
                tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in batch['input_ids']]
                
                # æ¨¡å‹é¢„æµ‹
                pred = model(batch['input_ids'], batch['attention_mask'], 
                             batch.get('token_type_ids', None))
                
                # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
                for i in range(batch['input_ids'].size(0)):
                    label_ids = batch['labels'][i].tolist()
                    # æ ¹æ®æ¨¡å‹è¾“å‡ºç±»å‹è·å–é¢„æµ‹çš„æ ‡ç­¾ID
                    if isinstance(pred, list):
                        pred_ids = pred[i]
                    else:
                        pred_ids = pred[i].argmax(dim=-1).tolist()
                    
                    sample_tokens = tokens[i]
                    word_labels = []
                    word_preds = []
                    words = []
                    
                    for token, true_id, pred_id in zip(sample_tokens, label_ids, pred_ids):
                        if token in ["[CLS]", "[SEP]", "[PAD]"] or token.startswith("<") or true_id == -100:
                            continue
                        
                        # ä¿å­˜å®é™…çš„token
                        words.append(token.replace("##", ""))
                        word_labels.append(id2label[true_id])
                        word_preds.append(id2label[pred_id])
                    
                    if word_labels:
                        true_labels.append(word_labels)
                        pred_labels.append(word_preds)
                        all_sentences.append(words)
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        precision = precision_score(true_labels, pred_labels)
        recall = recall_score(true_labels, pred_labels)
        f1 = f1_score(true_labels, pred_labels)
        
        logger.info(f"æ•´ä½“æ€§èƒ½: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
        print(f"æ•´ä½“æ€§èƒ½: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report = classification_report(true_labels, pred_labels, digits=4, output_dict=True)
        
        # æ‰“å°æ¯ç§å®ä½“ç±»å‹çš„è¯¦ç»†æŒ‡æ ‡
        entity_metrics = {}
        print("\nå®ä½“ç±»å‹æ€§èƒ½æŒ‡æ ‡:")
        logger.info("\nå®ä½“ç±»å‹æ€§èƒ½æŒ‡æ ‡:")
        for entity_type, metrics in report.items():
            if entity_type != "micro avg" and entity_type != "macro avg" and entity_type != "weighted avg" and isinstance(metrics, dict):
                entity_p = metrics['precision']
                entity_r = metrics['recall']
                entity_f1 = metrics['f1-score']
                support = metrics['support']
                entity_metrics[entity_type] = {
                    'precision': float(entity_p),
                    'recall': float(entity_r),
                    'f1': float(entity_f1),
                    'support': int(support)
                }
                print(f"  {entity_type}: P={entity_p:.4f}, R={entity_r:.4f}, F1={entity_f1:.4f}, æ ·æœ¬æ•°={support}")
                logger.info(f"  {entity_type}: P={entity_p:.4f}, R={entity_r:.4f}, F1={entity_f1:.4f}, æ ·æœ¬æ•°={support}")
        
        # è®¡ç®—æ¯ä¸ªéOæ ‡ç­¾çš„å®ä½“ç±»å‹
        entity_types = []
        for label in label_list:
            if label.startswith("B-") or label.startswith("I-"):
                entity_type = label[2:]  # å»æ‰"B-"æˆ–"I-"å‰ç¼€
                if entity_type not in entity_types:
                    entity_types.append(entity_type)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        results = {
            'model_info': {
                'pretrained_model': args.pretrained_model,
                'model_signature': model_signature,
                'model_path': model_path,
                'use_attention': args.use_attention,
                'use_bilstm': use_bilstm
            },
            'eval_info': {
                'test_file': args.test_file,
                'num_samples': len(test_dataset),
                'timestamp': timestamp
            },
            'metrics': {
                'overall': {
                    'precision': float(precision),
                    'recall': float(recall),
                    'f1': float(f1)
                },
                'entity_metrics': entity_metrics
            }
        }
        
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿç±»å‹
        results = convert_to_native_python_types(results)
        
        # ä¿å­˜JSONç»“æœ
        try:
            results_file = os.path.join(eval_dir, 'evaluation_results.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {results_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONç»“æœæ—¶å‡ºé”™: {e}")
            print(f"âŒ ä¿å­˜JSONç»“æœæ—¶å‡ºé”™: {e}")
        
        # å¦‚æœè¦ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        if args.detailed_report:
            report_dir = os.path.join(args.output_dir, safe_model_id, "reports")
            os.makedirs(report_dir, exist_ok=True)
            
            try:
                detailed_report_file = os.path.join(report_dir, 'detailed_classification_report.txt')
                with open(detailed_report_file, 'w', encoding='utf-8') as f:
                    f.write(classification_report(true_labels, pred_labels, digits=4))
                logger.info(f"è¯¦ç»†åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜è‡³: {detailed_report_file}")
            except Exception as e:
                logger.error(f"ä¿å­˜è¯¦ç»†åˆ†ç±»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
                print(f"âŒ ä¿å­˜è¯¦ç»†åˆ†ç±»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        
        # å¦‚æœè¦ä¿å­˜é¢„æµ‹ç»“æœ
        if args.save_predictions:
            try:
                predictions = []
                for i, (words, true_ls, pred_ls) in enumerate(zip(all_sentences, true_labels, pred_labels)):
                    predictions.append({
                        'id': i,
                        'words': words,
                        'true_labels': true_ls,
                        'pred_labels': pred_ls
                    })
                
                # ç¡®ä¿é¢„æµ‹ç»“æœä¹Ÿä½¿ç”¨PythonåŸç”Ÿç±»å‹
                predictions = convert_to_native_python_types(predictions)
                
                pred_file = os.path.join(eval_dir, 'predictions.json')
                with open(pred_file, 'w', encoding='utf-8') as f:
                    json.dump(predictions, f, ensure_ascii=False, indent=4)
                logger.info(f"é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {pred_file}")
            except Exception as e:
                logger.error(f"ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
                print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
        
        # ç»˜åˆ¶æ€§èƒ½æ¡å½¢å›¾
        try:
            plt.figure(figsize=(12, 6))
            metrics_df = pd.DataFrame([
                {'Entity Type': 'Overall', 'Precision': float(precision), 'Recall': float(recall), 'F1': float(f1)}
            ])
            
            for entity_type, metrics in entity_metrics.items():
                metrics_df = pd.concat([metrics_df, pd.DataFrame([{
                    'Entity Type': entity_type,
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1': metrics['f1']
                }])], ignore_index=True)
            
            metrics_melted = pd.melt(metrics_df, id_vars=['Entity Type'], 
                                    value_vars=['Precision', 'Recall', 'F1'],
                                    var_name='Metric', value_name='Score')
            
            # ä½¿ç”¨seabornçš„è®¾ç½®æé«˜å›¾è¡¨ç¾è§‚åº¦
            sns.set_style("whitegrid")
            # è®¾ç½®é¢œè‰²
            colors = sns.color_palette("muted", 3)
            # ç»˜åˆ¶æ¡å½¢å›¾
            ax = sns.barplot(data=metrics_melted, x='Entity Type', y='Score', hue='Metric', palette=colors)
            
            # ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
            plt.title('NER Performance Metrics', fontsize=14, fontweight='bold')
            plt.xlabel('Entity Types', fontsize=12)
            plt.ylabel('Score', fontsize=12)
            plt.ylim(0, 1.0)
            plt.xticks(rotation=45, ha='right')
            plt.legend(title='Metrics', fontsize=10)
            plt.grid(axis='y', linestyle='--', alpha=0.7)
            plt.tight_layout()
            
            # æ·»åŠ ä¸­æ–‡æ ‡é¢˜çš„è¯´æ˜ï¼ˆå¯é€‰ï¼‰
            plt.figtext(0.5, 0.01, "å‘½åå®ä½“è¯†åˆ«è¯„ä¼°ç»“æœ", ha="center", fontsize=10, alpha=0.7)
            
            chart_path = os.path.join(eval_dir, 'entity_performance.png')
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            logger.info(f"æ€§èƒ½æ¡å½¢å›¾å·²ä¿å­˜è‡³: {chart_path}")
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æ€§èƒ½æ¡å½¢å›¾æ—¶å‡ºé”™: {e}")
            print(f"âŒ ç»˜åˆ¶æ€§èƒ½æ¡å½¢å›¾æ—¶å‡ºé”™: {e}")
        
        # å¦‚æœéœ€è¦æ··æ·†çŸ©é˜µ
        if args.confusion_matrix and entity_types:
            cm_dir = os.path.join(args.output_dir, safe_model_id, "confusion_matrices")
            os.makedirs(cm_dir, exist_ok=True)
            
            try:
                # å‡†å¤‡æ··æ·†çŸ©é˜µæ•°æ®
                all_true_flat = []
                all_pred_flat = []
                
                for true_ls, pred_ls in zip(true_labels, pred_labels):
                    # æå–å®ä½“ç±»å‹ï¼ˆå»æ‰BIOå‰ç¼€ï¼‰
                    true_entity_types = ['O' if label == 'O' else label[2:] for label in true_ls]
                    pred_entity_types = ['O' if label == 'O' else label[2:] for label in pred_ls]
                    
                    all_true_flat.extend(true_entity_types)
                    all_pred_flat.extend(pred_entity_types)
                
                # è·å–æ‰€æœ‰å¯èƒ½çš„å®ä½“ç±»å‹ï¼ˆåŒ…æ‹¬Oï¼‰
                unique_entity_types = ['O'] + entity_types
                
                # æ„å»ºæ··æ·†çŸ©é˜µ
                confusion = np.zeros((len(unique_entity_types), len(unique_entity_types)))
                entity_to_idx = {entity: idx for idx, entity in enumerate(unique_entity_types)}
                
                for true_type, pred_type in zip(all_true_flat, all_pred_flat):
                    confusion[entity_to_idx[true_type]][entity_to_idx[pred_type]] += 1
                
                # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
                plt.figure(figsize=(10, 8))
                sns.set_style("white")
                
                # ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
                ax = sns.heatmap(confusion, annot=True, fmt='g', cmap='Blues',
                           xticklabels=unique_entity_types, yticklabels=unique_entity_types)
                
                plt.xlabel('Predicted Types', fontsize=12)
                plt.ylabel('True Types', fontsize=12)
                plt.title('Entity Type Confusion Matrix', fontsize=14, fontweight='bold')
                
                # æ·»åŠ ä¸­æ–‡æ ‡é¢˜çš„è¯´æ˜ï¼ˆå¯é€‰ï¼‰
                plt.figtext(0.5, 0.01, "å®ä½“ç±»å‹æ··æ·†çŸ©é˜µ", ha="center", fontsize=10, alpha=0.7)
                
                plt.tight_layout()
                
                confusion_path = os.path.join(cm_dir, 'confusion_matrix.png')
                plt.savefig(confusion_path, dpi=300, bbox_inches='tight')
                logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {confusion_path}")
            except Exception as e:
                logger.error(f"ç”Ÿæˆæ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")
                print(f"âŒ ç”Ÿæˆæ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³ {eval_dir}")
        logger.info(f"è¯„ä¼°å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³ {eval_dir}")
        
        return precision, recall, f1, report

    # åªæœ‰å½“è„šæœ¬ç›´æ¥è¿è¡Œæ—¶æ‰æ‰§è¡Œevaluateå‡½æ•°
    # å¹¶ä¸”åªæ‰§è¡Œä¸€æ¬¡
    evaluate()