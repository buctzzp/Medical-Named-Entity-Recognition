import os
import torch
import logging
from torch.utils.data import DataLoader
from transformers import BertTokenizerFast, get_scheduler
from model.bert_crf_model import BertCRF
from utils import NERDataset, get_label_list
import matplotlib.pyplot as plt
from tqdm import tqdm
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
from config import model_config
from datetime import datetime

print("âœ… æ˜¯å¦æ£€æµ‹åˆ° GPU:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ–¥ï¸ å½“å‰ GPU åç§°:", torch.cuda.get_device_name(0))
    print("ğŸ”¥ å½“å‰è®¾å¤‡:", torch.device("cuda"))
else:
    print("âŒ å½“å‰ä½¿ç”¨çš„æ˜¯ CPU")

# è®¾ç½®æ—¥å¿—
log_dir = model_config['log_dir']
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, f'train_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')),
        logging.StreamHandler() 
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½åˆ†è¯å™¨å’Œæ•°æ®
tokenizer = BertTokenizerFast.from_pretrained(model_config['bert_model_name'])

# æ„å»ºæ ‡ç­¾æ˜ å°„
label_list = get_label_list([
    model_config['train_path'],
    model_config['dev_path'],
    model_config['test_path']
])
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}
num_labels = len(label_list)

# åŠ è½½æ•°æ®é›†
train_dataset = NERDataset(model_config['train_path'], tokenizer, label2id, model_config['max_len'])
dev_dataset = NERDataset(model_config['dev_path'], tokenizer, label2id, model_config['max_len'])

train_loader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=model_config['eval_batch_size'])

# åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertCRF(model_config['bert_model_name'], num_labels).to(device)

# ä¼˜åŒ–å™¨é…ç½®
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=model_config['learning_rate'],
    weight_decay=model_config['weight_decay']
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦é¢„çƒ­ï¼‰
num_training_steps = len(train_loader) * model_config['num_epochs']
num_warmup_steps = int(num_training_steps * model_config['warmup_ratio'])
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

# éªŒè¯é›†è¯„ä¼°å‡½æ•°
def evaluate_on_dev():
    model.eval()
    true_labels = []
    pred_labels = []
    with torch.no_grad():
        for batch in dev_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            pred = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])
            
            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            for i in range(batch['input_ids'].size(0)):
                label_ids = batch['labels'][i].tolist()
                pred_ids = pred[i]
                
                tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])
                word_labels = []
                word_preds = []
                
                for token, true_id, pred_id in zip(tokens, label_ids, pred_ids):
                    if token in ["[CLS]", "[SEP]", "[PAD]"] or true_id == -100:
                        continue
                    word_labels.append(id2label[true_id])
                    word_preds.append(id2label[pred_id])
                
                if word_labels:
                    true_labels.append(word_labels)
                    pred_labels.append(word_preds)

    p = precision_score(true_labels, pred_labels)
    r = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)
    return p, r, f1

# è®­ç»ƒå¾ªç¯
best_f1 = 0
loss_history = []
early_stop_counter = 0

for epoch in range(model_config['num_epochs']):
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{model_config['num_epochs']}")
    
    for step, batch in enumerate(progress_bar):
        batch = {k: v.to(device) for k, v in batch.items()}
        loss = model(**batch)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), model_config['max_grad_norm'])
        
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        total_loss += loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'})
        
        # å®šæœŸè®°å½•è®­ç»ƒä¿¡æ¯
        if (step + 1) % model_config['logging_steps'] == 0:
            logger.info(
                f'Epoch: {epoch+1}/{model_config["num_epochs"]}, '
                f'Step: {step+1}/{len(train_loader)}, '
                f'Loss: {loss.item():.4f}, '
                f'LR: {optimizer.param_groups[0]["lr"]:.2e}'
            )

    avg_loss = total_loss / len(train_loader)
    loss_history.append(avg_loss)
    print(f"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}")

    # éªŒè¯é›†è¯„ä¼°
    p, r, f1 = evaluate_on_dev()
    print(f"\tDev Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}")
    if f1 > best_f1:
        best_f1 = f1
        torch.save(model.state_dict(), "checkpoints/best_model.pth")
        print("\tâœ… æ–°æœ€ä½³æ¨¡å‹ï¼Œå·²ä¿å­˜è‡³ checkpoints/best_model.pth")
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        print(f"\tâš ï¸ æœªæå‡ï¼ŒEarlyStopping è®¡æ•°: {early_stop_counter}/{patience}")
        if early_stop_counter >= patience:
            print("\nğŸ›‘ æå‰åœæ­¢è®­ç»ƒï¼ˆéªŒè¯é›† F1 æ— æå‡ï¼‰")
            break

# ä¿å­˜æœ€åæ¨¡å‹
torch.save(model.state_dict(), "checkpoints/bert_crf.pth")
print("\næ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ checkpoints/bert_crf.pth")

# å¯è§†åŒ–è®­ç»ƒæŸå¤±
plt.figure()
plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o')
plt.xlabel("Epoch")
plt.ylabel("Average Loss")
plt.title("Training Loss Over Epochs")
plt.grid(True)
plt.savefig("checkpoints/training_loss.png")
plt.show()