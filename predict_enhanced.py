# predict_enhanced.py â€” å¢å¼ºç‰ˆé¢„æµ‹è„šæœ¬ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œå¯è§†åŒ–åˆ†æ

import os
import torch
import argparse
import json
from transformers import BertTokenizerFast
from model.model_factory import ModelFactory
from utils import get_label_list
from model_explainability import ModelExplainer
from config import model_config, explainability_config
from tqdm import tqdm
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging
import time
from model.bert_crf_model import BertCRF
from model.bert_attention_crf_model import BertAttentionCRF

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description='ä¸­æ–‡åŒ»ç–—NERæ¨¡å‹é¢„æµ‹å·¥å…·')
parser.add_argument('--input', type=str, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªå¥å­ï¼Œä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼')
parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ')
parser.add_argument('--model', type=str, default=model_config['best_model_path'], help='æ¨¡å‹æƒé‡è·¯å¾„')
parser.add_argument('--format', type=str, choices=['json', 'text', 'bio'], default='json', help='è¾“å‡ºæ ¼å¼ï¼šjson, text, bio')
parser.add_argument('--batch_size', type=int, default=model_config['eval_batch_size'], help='æ‰¹å¤„ç†å¤§å°')
parser.add_argument('--pretrained_model', type=str, default=model_config['pretrained_model_name'], 
                    help='é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œå¯é€‰ï¼šbert-base-chinese, chinese-medical-bert, pcl-medbert, cmeee-bert, mc-bert, chinese-roberta-med')
parser.add_argument('--use_attention', action='store_true', default=model_config.get('use_attention', False), help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æ¨¡å‹')
parser.add_argument('--use_bilstm', action='store_true', default=model_config.get('use_bilstm', False), help='æ˜¯å¦ä½¿ç”¨BiLSTMå±‚')
parser.add_argument('--no_bilstm', action='store_true', help='ç¦ç”¨BiLSTMå±‚ï¼ˆè¦†ç›–é»˜è®¤é…ç½®ï¼‰')
parser.add_argument('--label_path', type=str, default=None, help='æ ‡ç­¾æ˜ å°„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›å°†ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾')
parser.add_argument('--max_length', type=int, default=model_config['max_len'], help='æœ€å¤§åºåˆ—é•¿åº¦')
parser.add_argument('--pretty', action='store_true', default=True, help='ç¾åŒ–JSONè¾“å‡º')
parser.add_argument('--detailed', action='store_true', help='è¾“å‡ºè¯¦ç»†çš„å®ä½“è¯†åˆ«ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½ç½®å’Œç±»å‹æ¦‚ç‡')
args = parser.parse_args()

# no_bilstm æ¯” use_bilstm ä¼˜å…ˆçº§é«˜
use_bilstm = args.use_bilstm
if args.no_bilstm:
    use_bilstm = False

# ç”Ÿæˆæ¨¡å‹IDå’Œç­¾åï¼Œä¸train_enhanced.pyä¿æŒä¸€è‡´
model_id = args.pretrained_model.replace('-', '_')
model_type = "attention" if args.use_attention else "base"
bilstm_status = "with_bilstm" if use_bilstm else "no_bilstm"
model_signature = f"{model_id}_{model_type}_{bilstm_status}"

# è®¾ç½®è¾“å‡ºç›®å½•
if args.output:
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if not output_dir:
        output_dir = '.'
    os.makedirs(output_dir, exist_ok=True)
    output_file = args.output
else:
    # ä½¿ç”¨æ—¶é—´æˆ³å’Œæ¨¡å‹ä¿¡æ¯ç”Ÿæˆè¾“å‡ºç›®å½•å’Œæ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = os.path.join('results', model_id)
    os.makedirs(results_dir, exist_ok=True)
    output_file = os.path.join(results_dir, f"predict_{model_signature}_{timestamp}.{args.format}")

# è®¾ç½®æ—¥å¿—ç›®å½•
log_dir = os.path.join(model_config['log_dir'], model_id)
os.makedirs(log_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—è¾“å‡º
log_file = os.path.join(log_dir, f'prediction_{model_signature}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®°å½•é¢„æµ‹é…ç½®
logger.info("="*50)
logger.info("åŒ»å­¦å‘½åå®ä½“è¯†åˆ«é¢„æµ‹å¼€å§‹")
logger.info("="*50)
logger.info(f"ğŸš€ é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
logger.info(f"ğŸ” æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
logger.info(f"ğŸ§  BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
logger.info(f"ğŸ“Š æ¨¡å‹ç­¾å: {model_signature}")
logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
logger.info(f"è¾“å…¥: {'äº¤äº’æ¨¡å¼' if not args.input else args.input}")
logger.info(f"è¾“å‡ºæ ¼å¼: {args.format}")
logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
logger.info(f"æœ€å¤§åºåˆ—é•¿åº¦: {args.max_length}")
logger.info(f"è¯¦ç»†è¾“å‡º: {args.detailed}")
logger.info("="*50)

# æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹è·¯å¾„
model_dir = os.path.join(model_config['model_dir'], model_id)
os.makedirs(model_dir, exist_ok=True)

if args.model == 'best':
    model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = model_config['best_model_path']  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'final':
    model_path = os.path.join(model_dir, f"final_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"final_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = model_config['final_model_path']  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'quantized':
    model_path = os.path.join(model_dir, f"quantized_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"quantized_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, "quantized_model.pth")  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'pruned':
    model_path = os.path.join(model_dir, f"pruned_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"pruned_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            logger.warning(f"è­¦å‘Š: å‰ªææ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹ä»£æ›¿")
            model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
            if not os.path.exists(model_path):
                model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
                if not os.path.exists(model_path):
                    model_path = model_config['best_model_path']  # å†æ¬¡å›é€€
else:
    # ç”¨æˆ·æŒ‡å®šçš„å…·ä½“è·¯å¾„
    model_path = args.model

logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model_path}")

# åŠ è½½æ ‡ç­¾åˆ—è¡¨
if args.label_path and os.path.exists(args.label_path):
    # ä»æ–‡ä»¶åŠ è½½æ ‡ç­¾
    logger.info(f"ä»æ–‡ä»¶åŠ è½½æ ‡ç­¾: {args.label_path}")
    with open(args.label_path, 'r', encoding='utf-8') as f:
        label_list = json.load(f)
else:
    # ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾
    logger.info("ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾åˆ—è¡¨")
    train_paths = [model_config['train_path']]
    if os.path.exists(model_config['dev_path']):
        train_paths.append(model_config['dev_path'])
    if os.path.exists(model_config['test_path']):
        train_paths.append(model_config['test_path'])
    label_list = get_label_list(train_paths)

# æ„å»ºæ ‡ç­¾æ˜ å°„
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}
num_labels = len(label_list)

logger.info(f"æ ‡ç­¾æ€»æ•°: {num_labels}")
logger.info(f"æ ‡ç­¾åˆ—è¡¨: {label_list}")

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

# ä½¿ç”¨æ¨¡å‹å·¥å‚åˆ›å»ºæ¨¡å‹å’Œè·å–åˆ†è¯å™¨
tokenizer = ModelFactory.get_tokenizer_for_model(args.pretrained_model)
  
# æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹ç±»å‹
if args.use_attention:
    model = BertAttentionCRF.from_pretrained(
        args.pretrained_model,
        num_labels=num_labels,
        use_bilstm=use_bilstm,
        lstm_hidden_size=model_config['lstm_hidden_size'],
        lstm_layers=model_config['lstm_layers'],
        lstm_dropout=model_config['lstm_dropout'],
        attention_size=model_config['attention_size'],
        num_attention_heads=model_config['num_attention_heads'],
        attention_dropout=model_config['attention_dropout'],
        hidden_dropout=model_config['hidden_dropout']
    )
else:
    model = BertCRF.from_pretrained(
        args.pretrained_model,
        num_labels=num_labels,
        use_bilstm=use_bilstm,
        lstm_hidden_size=model_config['lstm_hidden_size'],
        lstm_layers=model_config['lstm_layers'],
        lstm_dropout=model_config['lstm_dropout']
    )

# åŠ è½½é¢„è®­ç»ƒæƒé‡
try:
    model.load_state_dict(torch.load(model_path, map_location=device))
    logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²ç»ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒäº†æ¨¡å‹")
    exit(1)

model.to(device)
model.eval()

# åˆå§‹åŒ–æ¨¡å‹è§£é‡Šå™¨
explainer = ModelExplainer(model, tokenizer, id2label, device)

# è¯»å–è¾“å…¥æ–‡ä»¶
if args.input and os.path.exists(args.input):
    with open(args.input, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
    print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
elif args.input and not os.path.exists(args.input):
    logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    texts = [args.input]  # å°†è¾“å…¥å‚æ•°ä½œä¸ºå•ä¸ªæ–‡æœ¬æ ·æœ¬
    logger.info("å°†å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºå•ä¸ªæ–‡æœ¬æ ·æœ¬å¤„ç†")
    print("ğŸ”¤ å°†å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºå•ä¸ªæ–‡æœ¬æ ·æœ¬å¤„ç†")
else:
    # äº¤äº’æ¨¡å¼
    logger.info("è¿›å…¥äº¤äº’æ¨¡å¼")
    print("ğŸ“ è¯·è¾“å…¥æ–‡æœ¬è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆè¾“å…¥'quit'æˆ–'exit'é€€å‡ºï¼‰ï¼š")
    texts = []
    while True:
        user_input = input(">> ")
        if user_input.lower() in ["quit", "exit", "q"]:
            break
        texts.append(user_input)
    
    if not texts:
        logger.info("æœªè¾“å…¥ä»»ä½•æ–‡æœ¬ï¼Œé€€å‡º")
        print("âŒ æœªè¾“å…¥ä»»ä½•æ–‡æœ¬ï¼Œé€€å‡º")
        exit(0)
    
    logger.info(f"äº¤äº’æ¨¡å¼æ”¶é›†äº† {len(texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")

# é¢„æµ‹å‡½æ•°
def predict_batch(batch_texts):
    encoded_inputs = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=args.max_length,
        return_tensors="pt"
    )
    
    input_ids = encoded_inputs["input_ids"].to(device)
    attention_mask = encoded_inputs["attention_mask"].to(device)
    token_type_ids = encoded_inputs.get("token_type_ids", None)
    if token_type_ids is not None:
        token_type_ids = token_type_ids.to(device)
    
    with torch.no_grad():
        if token_type_ids is not None:
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        else:
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            
    # å¤„ç†ä¸åŒæ¨¡å‹è¾“å‡ºæ ¼å¼  
    if isinstance(outputs, list):
        # CRFæ¨¡å‹ç›´æ¥è¿”å›æ ‡ç­¾åºåˆ—
        predicted_label_ids = outputs
    else:
        # éCRFæ¨¡å‹ï¼Œéœ€è¦ä»logitsä¸­è·å–æ ‡ç­¾
        predicted_label_ids = outputs.argmax(dim=2).cpu().numpy()
    
    # å¤„ç†ç»“æœ
    batch_results = []
    for i, text in enumerate(batch_texts):
        text_tokens = tokenizer.convert_ids_to_tokens(input_ids[i])
        
        # æ ¹æ®æ¨¡å‹è¾“å‡ºç±»å‹è·å–é¢„æµ‹çš„æ ‡ç­¾ID
        if isinstance(outputs, list):
            pred_label_ids = outputs[i] 
        else:
            pred_label_ids = predicted_label_ids[i]
        
        # å¤„ç†æ¯ä¸ªtoken
        result_labels = []
        entities = []
        current_entity = None
        
        for j, (token, pred_id) in enumerate(zip(text_tokens, pred_label_ids)):
            if token in ["[CLS]", "[SEP]", "[PAD]"] or token.startswith("<"):
                # å¦‚æœå½“å‰æœ‰æ­£åœ¨å¤„ç†çš„å®ä½“ï¼Œç»“æŸå®ƒ
                if current_entity is not None:
                    entities.append(current_entity)
                    current_entity = None
                continue
                
            # æ¢å¤åŸå§‹æ–‡æœ¬ (å¤„ç†WordPieceåˆ†è¯)
            token_text = token.replace("##", "")
            
            # è·å–é¢„æµ‹æ ‡ç­¾
            label = id2label[pred_id]
            result_labels.append(label)
            
            # å¤„ç†å®ä½“
            if label.startswith("B-"):
                # å¦‚æœå½“å‰æœ‰æ­£åœ¨å¤„ç†çš„å®ä½“ï¼Œç»“æŸå®ƒ
                if current_entity is not None:
                    entities.append(current_entity)
                
                # å¼€å§‹ä¸€ä¸ªæ–°å®ä½“
                entity_type = label[2:]  # å»æ‰"B-"å‰ç¼€
                current_entity = {
                    "type": entity_type,
                    "text": token_text,
                    "start": j-1 if j > 0 else 0,  # è¿‘ä¼¼ä½ç½®ï¼Œéœ€è¦åå¤„ç†æ ¡æ­£
                    "end": j
                }
            elif label.startswith("I-") and current_entity is not None:
                # ç»§ç»­å½“å‰å®ä½“
                entity_type = label[2:]  # å»æ‰"I-"å‰ç¼€
                if entity_type == current_entity["type"]:
                    current_entity["text"] += token_text
                    current_entity["end"] = j
            elif current_entity is not None:
                # ç»“æŸå½“å‰å®ä½“
                entities.append(current_entity)
                current_entity = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
        if current_entity is not None:
            entities.append(current_entity)
        
        # æ ¡æ­£å®ä½“ä½ç½®
        corrected_entities = []
        for entity in entities:
            # åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾å®ä½“æ–‡æœ¬ï¼Œæ‰¾åˆ°å®é™…ä½ç½®
            entity_text = entity["text"]
            entity_type = entity["type"]
            
            # å¦‚æœå®ä½“æ–‡æœ¬åœ¨åŸå§‹æ–‡æœ¬ä¸­èƒ½æ‰¾åˆ°ï¼Œä½¿ç”¨å®é™…ä½ç½®
            start_pos = text.find(entity_text)
            if start_pos != -1:
                corrected_entities.append({
                    "type": entity_type,
                    "text": entity_text,
                    "start": start_pos,
                    "end": start_pos + len(entity_text)
                })
            else:
                # å¦åˆ™ä½¿ç”¨è¿‘ä¼¼ä½ç½®
                corrected_entities.append(entity)
        
        # æ·»åŠ åˆ°æ‰¹æ¬¡ç»“æœ
        batch_results.append({
            "text": text,
            "labels": result_labels,
            "entities": corrected_entities
        })
    
    return batch_results

# åˆ†æ‰¹å¤„ç†æ–‡æœ¬
results = []
batch_size = args.batch_size
num_batches = (len(texts) + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´

start_time = time.time()
for i in tqdm(range(num_batches), desc="é¢„æµ‹è¿›åº¦"):
    start_idx = i * batch_size
    end_idx = min(start_idx + batch_size, len(texts))
    batch_texts = texts[start_idx:end_idx]
    
    batch_results = predict_batch(batch_texts)
    results.extend(batch_results)

end_time = time.time()
processing_time = end_time - start_time
avg_time_per_sample = processing_time / len(texts)

logger.info(f"é¢„æµ‹å®Œæˆ! å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’, æ¯æ ·æœ¬å¹³å‡: {avg_time_per_sample:.4f}ç§’")
print(f"âœ… é¢„æµ‹å®Œæˆ! å¤„ç† {len(texts)} ä¸ªæ ·æœ¬ç”¨æ—¶ {processing_time:.2f}ç§’, å¹³å‡æ¯æ ·æœ¬ {avg_time_per_sample:.4f}ç§’")

# æ ¹æ®è¾“å‡ºæ ¼å¼ä¿å­˜ç»“æœ
if args.format == 'json':
    # JSONæ ¼å¼è¾“å‡º - åŒ…å«å®Œæ•´çš„å®ä½“ä¿¡æ¯
    output_data = []
    for result in results:
        # å¦‚æœä¸éœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œåªä¿ç•™å¿…è¦çš„å­—æ®µ
        if not args.detailed:
            output_data.append({
                "text": result["text"],
                "entities": [
                    {"type": e["type"], "text": e["text"]} 
                    for e in result["entities"]
                ]
            })
        else:
            output_data.append(result)
    
    # å†™å…¥JSONæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        if args.pretty:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        else:
            json.dump(output_data, f, ensure_ascii=False)
            
elif args.format == 'text':
    # æ–‡æœ¬æ ¼å¼è¾“å‡º - æ¯ä¸ªæ ·æœ¬ä¸€è¡Œï¼Œå¸¦æœ‰æ ‡æ³¨çš„å®ä½“
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            text = result["text"]
            entities = result["entities"]
            
            # æŒ‰ç…§ç»“æŸä½ç½®ä»å¤§åˆ°å°æ’åºï¼Œä»¥ä¾¿ä»åå‘å‰å¤„ç†æ–‡æœ¬
            entities = sorted(entities, key=lambda e: e["end"], reverse=True)
            
            # åœ¨æ–‡æœ¬ä¸­æ ‡è®°å®ä½“
            marked_text = text
            for entity in entities:
                start = entity["start"]
                end = entity["end"]
                entity_type = entity["type"]
                
                # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°çªå‡ºæ˜¾ç¤ºå®ä½“
                marked_text = (
                    marked_text[:start] + 
                    f"[{marked_text[start:end]}:{entity_type}]" + 
                    marked_text[end:]
                )
            
            f.write(f"{marked_text}\n")
            
elif args.format == 'bio':
    # BIOæ ¼å¼è¾“å‡º - æ¯è¡Œä¸€ä¸ªå­—ç¬¦å’Œå¯¹åº”çš„BIOæ ‡ç­¾
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            text = result["text"]
            entities = result["entities"]
            
            # åˆ›å»ºé»˜è®¤æ ‡ç­¾(å…¨éƒ¨ä¸ºO)
            bio_tags = ["O"] * len(text)
            
            # ä¸ºæ‰€æœ‰å®ä½“åˆ†é…BIOæ ‡ç­¾
            for entity in entities:
                start = entity["start"]
                end = entity["end"]
                entity_type = entity["type"]
                
                # åˆ†é…Bæ ‡ç­¾ç»™å®ä½“çš„ç¬¬ä¸€ä¸ªå­—ç¬¦
                bio_tags[start] = f"B-{entity_type}"
                
                # åˆ†é…Iæ ‡ç­¾ç»™å®ä½“çš„å‰©ä½™å­—ç¬¦
                for i in range(start + 1, end):
                    bio_tags[i] = f"I-{entity_type}"
            
            # è¾“å‡ºå­—ç¬¦å’Œå¯¹åº”çš„BIOæ ‡ç­¾
            for char, tag in zip(text, bio_tags):
                f.write(f"{char} {tag}\n")
            f.write("\n")  # ä¸åŒæ ·æœ¬ä¹‹é—´çš„ç©ºè¡Œ

logger.info(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

# å¯è§†åŒ–ç»“æœç»Ÿè®¡ï¼ˆå¦‚æœæ ·æœ¬æ•°è¶…è¿‡1ï¼‰
if len(results) > 1:
    # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
    entity_types = {}
    for result in results:
        for entity in result["entities"]:
            entity_type = entity["type"]
            if entity_type not in entity_types:
                entity_types[entity_type] = 0
            entity_types[entity_type] += 1
    
    # ç”Ÿæˆç»Ÿè®¡å›¾è¡¨
    if entity_types:
        plt.figure(figsize=(10, 6))
        plt.bar(entity_types.keys(), entity_types.values())
        plt.title('å®ä½“ç±»å‹åˆ†å¸ƒ')
        plt.xlabel('å®ä½“ç±»å‹')
        plt.ylabel('æ•°é‡')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = os.path.splitext(output_file)[0] + "_entity_distribution.png"
        plt.savefig(chart_path)
        logger.info(f"å®ä½“ç±»å‹åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {chart_path}")
        print(f"ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {chart_path}")