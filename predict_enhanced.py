# predict_enhanced.py â€” å¢å¼ºç‰ˆé¢„æµ‹è„šæœ¬ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œå¯è§†åŒ–åˆ†æ

import os
import torch
import argparse
import json
from transformers import BertTokenizerFast
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from datetime import datetime
import logging
import time
from model.model_factory import ModelFactory
from utils import get_label_list, clean_entity_name
from config import model_config

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description='ä¸­æ–‡åŒ»ç–—NERæ¨¡å‹é¢„æµ‹å·¥å…·')
parser.add_argument('--input', type=str, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªå¥å­ï¼Œä¸æä¾›åˆ™è¿›å…¥äº¤äº’æ¨¡å¼')
parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ')
parser.add_argument('--model', type=str, default=model_config['best_model_path'], help='æ¨¡å‹æƒé‡è·¯å¾„')
parser.add_argument('--format', type=str, choices=['json', 'text', 'bio'], default='json', help='è¾“å‡ºæ ¼å¼ï¼šjson, text, bio')
parser.add_argument('--batch_size', type=int, default=model_config['eval_batch_size'], help='æ‰¹å¤„ç†å¤§å°')
parser.add_argument('--pretrained_model', type=str, default=model_config['pretrained_model_name'], 
                    help='é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œå¯é€‰ï¼šbert-base-chinese, chinese-medical-bert, pcl-medbert, cmeee-bert, mc-bert, chinese-roberta-med')
parser.add_argument('--use_attention', action='store_true', default=model_config.get('use_attention', False), help='æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æ¨¡å‹')
parser.add_argument('--use_bilstm', action='store_true', default=model_config.get('use_bilstm', False), help='æ˜¯å¦ä½¿ç”¨BiLSTMå±‚')
parser.add_argument('--no_bilstm', action='store_true', help='ç¦ç”¨BiLSTMå±‚ï¼ˆè¦†ç›–é»˜è®¤é…ç½®ï¼‰')
parser.add_argument('--label_path', type=str, default=None, help='æ ‡ç­¾æ˜ å°„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›å°†ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾')
parser.add_argument('--max_length', type=int, default=model_config['max_len'], help='æœ€å¤§åºåˆ—é•¿åº¦')
parser.add_argument('--pretty', action='store_true', default=True, help='ç¾åŒ–JSONè¾“å‡º')
parser.add_argument('--detailed', action='store_true', help='è¾“å‡ºè¯¦ç»†çš„å®ä½“è¯†åˆ«ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½ç½®å’Œç±»å‹æ¦‚ç‡')
args = parser.parse_args()

# no_bilstm æ¯” use_bilstm ä¼˜å…ˆçº§é«˜
use_bilstm = args.use_bilstm
if args.no_bilstm:
    use_bilstm = False

# ç”Ÿæˆæ¨¡å‹IDå’Œç­¾åï¼Œä¸train_enhanced.pyä¿æŒä¸€è‡´
model_id = args.pretrained_model.replace('-', '_')
model_type = "attention" if args.use_attention else "base"
bilstm_status = "with_bilstm" if use_bilstm else "no_bilstm"
model_signature = f"{model_id}_{model_type}_{bilstm_status}"

# è®¾ç½®è¾“å‡ºç›®å½•
if args.output:
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if not output_dir:
        output_dir = '.'
    os.makedirs(output_dir, exist_ok=True)
    output_file = args.output
else:
    # ä½¿ç”¨æ—¶é—´æˆ³å’Œæ¨¡å‹ä¿¡æ¯ç”Ÿæˆè¾“å‡ºç›®å½•å’Œæ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = os.path.join('results', model_id)
    os.makedirs(results_dir, exist_ok=True)
    output_file = os.path.join(results_dir, f"predict_{model_signature}_{timestamp}.{args.format}")

# è®¾ç½®æ—¥å¿—ç›®å½•
log_dir = os.path.join(model_config['log_dir'], model_id)
os.makedirs(log_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—è¾“å‡º
log_file = os.path.join(log_dir, f'prediction_{model_signature}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # æŒ‡å®šç¼–ç ä¸ºutf-8
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®°å½•é¢„æµ‹é…ç½®
logger.info("="*50)
logger.info("åŒ»å­¦å‘½åå®ä½“è¯†åˆ«é¢„æµ‹å¼€å§‹")
logger.info("="*50)
logger.info(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
logger.info(f"æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
logger.info(f"BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
logger.info(f"æ¨¡å‹ç­¾å: {model_signature}")
logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
logger.info(f"è¾“å…¥: {'äº¤äº’æ¨¡å¼' if not args.input else args.input}")
logger.info(f"è¾“å‡ºæ ¼å¼: {args.format}")
logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
logger.info(f"æœ€å¤§åºåˆ—é•¿åº¦: {args.max_length}")
logger.info(f"è¯¦ç»†è¾“å‡º: {args.detailed}")
logger.info("="*50)

# æ§åˆ¶å°è¾“å‡ºå¯ä»¥ä¿ç•™emoji
print(f"ğŸš€ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
print(f"ğŸ” æ¨¡å‹ç±»å‹: {'BERT-Attention-CRF' if args.use_attention else 'BERT-CRF'}")
print(f"ğŸ§  BiLSTMå±‚: {'å¯ç”¨' if use_bilstm else 'ç¦ç”¨'}")
print(f"ğŸ“Š æ¨¡å‹ç­¾å: {model_signature}")
print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")

# æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹è·¯å¾„
model_dir = os.path.join(model_config['model_dir'], model_id)
os.makedirs(model_dir, exist_ok=True)

if args.model == 'best':
    model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = model_config['best_model_path']  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'final':
    model_path = os.path.join(model_dir, f"final_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"final_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = model_config['final_model_path']  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'quantized':
    model_path = os.path.join(model_dir, f"quantized_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"quantized_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            model_path = os.path.join(model_dir, "quantized_model.pth")  # å›é€€åˆ°é»˜è®¤è·¯å¾„
elif args.model == 'pruned':
    model_path = os.path.join(model_dir, f"pruned_model_{model_signature}.pth")
    if not os.path.exists(model_path):
        model_path = os.path.join(model_dir, f"pruned_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
        if not os.path.exists(model_path):
            logger.warning(f"è­¦å‘Š: å‰ªææ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹ä»£æ›¿")
            model_path = os.path.join(model_dir, f"best_model_{model_signature}.pth")
            if not os.path.exists(model_path):
                model_path = os.path.join(model_dir, f"best_model_{model_id}.pth")  # å…¼å®¹æ—§ç‰ˆè·¯å¾„
                if not os.path.exists(model_path):
                    model_path = model_config['best_model_path']  # å†æ¬¡å›é€€
else:
    # ç”¨æˆ·æŒ‡å®šçš„å…·ä½“è·¯å¾„
    model_path = args.model

logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model_path}")

# åŠ è½½æ ‡ç­¾åˆ—è¡¨
if args.label_path and os.path.exists(args.label_path):
    # ä»æ–‡ä»¶åŠ è½½æ ‡ç­¾
    logger.info(f"ä»æ–‡ä»¶åŠ è½½æ ‡ç­¾: {args.label_path}")
    with open(args.label_path, 'r', encoding='utf-8') as f:
        label_list = json.load(f)
else:
    # ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾
    logger.info("ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡ç­¾åˆ—è¡¨")
    train_paths = [model_config['train_path']]
    if os.path.exists(model_config['dev_path']):
        train_paths.append(model_config['dev_path'])
    if os.path.exists(model_config['test_path']):
        train_paths.append(model_config['test_path'])
    label_list = get_label_list(train_paths)

# æ„å»ºæ ‡ç­¾æ˜ å°„
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}
num_labels = len(label_list)

logger.info(f"æ ‡ç­¾æ€»æ•°: {num_labels}")
logger.info(f"æ ‡ç­¾åˆ—è¡¨: {label_list}")

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

# ä½¿ç”¨æ¨¡å‹å·¥å‚è·å–ä¸æ¨¡å‹åŒ¹é…çš„tokenizer
tokenizer = ModelFactory.get_tokenizer_for_model(args.pretrained_model)

# åŠ è½½æ¨¡å‹ - ä½¿ç”¨ModelFactoryæ­£ç¡®åˆ›å»ºæ¨¡å‹
model_type_str = 'bert_attention_crf' if args.use_attention else 'bert_crf'
model = ModelFactory.create_model(
    model_type=model_type_str,
    num_labels=num_labels,
    pretrained_model_name=args.pretrained_model
)

# è®¾ç½®æ¨¡å‹çš„BiLSTMå‚æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
if hasattr(model, 'use_bilstm'):
    model.use_bilstm = use_bilstm
    # æ›´æ–°BiLSTMç›¸å…³é…ç½®
    if hasattr(model, 'lstm_hidden_size'):
        model.lstm_hidden_size = model_config.get('lstm_hidden_size', 128)
    if hasattr(model, 'lstm_layers'):
        model.lstm_layers = model_config.get('lstm_layers', 1)
    if hasattr(model, 'lstm_dropout'):
        model.lstm_dropout = model_config.get('lstm_dropout', 0.1)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
try:
    model.load_state_dict(torch.load(model_path, map_location=device))
    logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²ç»ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒäº†æ¨¡å‹")
    exit(1)

model.to(device)
model.eval()

# è¯»å–è¾“å…¥æ–‡ä»¶
if args.input and os.path.exists(args.input):
    with open(args.input, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
    print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
else:
    # äº¤äº’æ¨¡å¼
    if not args.input:
        print("ğŸ–‹ï¸ æ²¡æœ‰æä¾›è¾“å…¥æ–‡ä»¶ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼")
        texts = []
        example_text = "æ‚£è€…å‡ºç°é«˜è¡€å‹å’Œ2å‹ç³–å°¿ç—…ï¼Œå»ºè®®æœç”¨é™å‹è¯ã€‚"
        print(f"ğŸ“ è¯·è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼ˆæ¯è¡Œä¸€å¥ï¼Œè¾“å…¥ç©ºè¡Œå¼€å§‹é¢„æµ‹ï¼Œè¾“å…¥'exit'é€€å‡ºï¼‰")
        print(f"ğŸ“ ç¤ºä¾‹: {example_text}")
        
        while True:
            line = input(">>> ").strip()
            if line.lower() == 'exit':
                if not texts:
                    print("ğŸ‘‹ å†è§!")
                    exit(0)
                else:
                    break
            elif not line and texts:
                break
            elif line:
                texts.append(line)
    else:
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        exit(1)

# æ‰¹é‡é¢„æµ‹å‡½æ•°
def predict_batch(batch_texts):
    encoded_input = tokenizer(
        batch_texts,
        padding=True,
        truncation=True,
        max_length=args.max_length,
        return_tensors="pt"
    )
    
    input_ids = encoded_input['input_ids'].to(device)
    attention_mask = encoded_input['attention_mask'].to(device)
    token_type_ids = encoded_input.get('token_type_ids', None)
    if token_type_ids is not None:
        token_type_ids = token_type_ids.to(device)
    
    # æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        outputs = model(input_ids, attention_mask, token_type_ids)
    
    # å¤„ç†é¢„æµ‹ç»“æœ
    batch_entities = []
    
    for i, text in enumerate(batch_texts):
        # è·å–ç¬¬iä¸ªæ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ID
        if isinstance(outputs, list):
            pred_ids = outputs[i]
        else:
            pred_ids = outputs[i].argmax(dim=-1).tolist()
        
        # å°†tokenæ˜ å°„å›åŸå§‹æ–‡æœ¬
        tokens = tokenizer.convert_ids_to_tokens(input_ids[i])
        
        entities = []
        entity = None
        orig_tokens = []
        
        # æ”¶é›†å®ä½“
        for j, (token, pred_id) in enumerate(zip(tokens, pred_ids)):
            if token in ['[CLS]', '[SEP]', '[PAD]'] or token.startswith('<'):
                continue
                
            token = token.replace('##', '')
            orig_tokens.append(token)
            
            pred_label = id2label.get(pred_id, 'O')
            
            if pred_label.startswith('B-'):
                if entity:
                    entity_text = ''.join(entity['tokens']).replace('##', '')
                    entity['text'] = entity_text
                    # æ¸…ç†å®ä½“åç§°
                    entity['text'] = clean_entity_name(entity['text'])
                    entities.append(entity)
                
                entity_type = pred_label[2:]  # ç§»é™¤"B-"å‰ç¼€
                entity = {
                    'type': entity_type,
                    'tokens': [token],
                    'start': len(''.join(orig_tokens[:-1])),  # å½“å‰å¤„ç†çš„tokençš„ä½ç½®å°±æ˜¯å®ä½“å¼€å§‹ä½ç½®
                    'end': len(''.join(orig_tokens))  # å½“å‰å¤„ç†tokenç»“æŸçš„ä½ç½®
                }
            elif pred_label.startswith('I-') and entity:
                # ç¡®ä¿I-æ ‡ç­¾ç±»å‹ä¸å½“å‰å®ä½“ç±»å‹ä¸€è‡´
                if pred_label[2:] == entity['type']:
                    entity['tokens'].append(token)
                    entity['end'] = len(''.join(orig_tokens))  # æ›´æ–°å®ä½“ç»“æŸä½ç½®
            elif pred_label == 'O':
                if entity:
                    entity_text = ''.join(entity['tokens']).replace('##', '')
                    entity['text'] = entity_text
                    # æ¸…ç†å®ä½“åç§°
                    entity['text'] = clean_entity_name(entity['text'])
                    entities.append(entity)
                    entity = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
        if entity:
            entity_text = ''.join(entity['tokens']).replace('##', '')
            entity['text'] = entity_text
            # æ¸…ç†å®ä½“åç§°
            entity['text'] = clean_entity_name(entity['text'])
            entities.append(entity)
        
        batch_entities.append({
            'text': text,
            'entities': entities
        })
    
    return batch_entities

# å¯¹æ‰€æœ‰æ–‡æœ¬è¿›è¡Œé¢„æµ‹
all_results = []
total_time = 0
total_entities = 0
start_time = time.time()

for i in range(0, len(texts), args.batch_size):
    batch_texts = texts[i:i+args.batch_size]
    batch_results = predict_batch(batch_texts)
    all_results.extend(batch_results)
    
    # ç»Ÿè®¡è¯†åˆ«åˆ°çš„å®ä½“
    for result in batch_results:
        total_entities += len(result['entities'])
    
    # è¿›åº¦æç¤º
    if len(texts) > args.batch_size:
        print(f"ğŸ“Š å·²å¤„ç† {min(i+args.batch_size, len(texts))}/{len(texts)} æ¡æ–‡æœ¬")

total_time = time.time() - start_time
logger.info(f"é¢„æµ‹å®Œæˆ: å¤„ç†äº† {len(texts)} æ¡æ–‡æœ¬, å…±è¯†åˆ« {total_entities} ä¸ªå®ä½“, è€—æ—¶ {total_time:.2f} ç§’")
print(f"âœ… é¢„æµ‹å®Œæˆ: å¤„ç†äº† {len(texts)} æ¡æ–‡æœ¬, å…±è¯†åˆ« {total_entities} ä¸ªå®ä½“, è€—æ—¶ {total_time:.2f} ç§’")

# æ ¹æ®è¾“å‡ºæ ¼å¼ç”Ÿæˆç»“æœ
if args.format == 'json':
    # è¾“å‡ºJSONæ ¼å¼
    json_results = []
    
    for result in all_results:
        json_result = {'text': result['text'], 'entities': []}
        
        for entity in result['entities']:
            entity_info = {
                'text': entity['text'],
                'type': entity['type'],
                'start': entity['start'],
                'end': entity['end']
            }
            json_result['entities'].append(entity_info)
        
        json_results.append(json_result)
    
    # ä¿å­˜JSONç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        if args.pretty:
            json.dump(json_results, f, ensure_ascii=False, indent=2)
        else:
            json.dump(json_results, f, ensure_ascii=False)
    
    # äº¤äº’æ¨¡å¼ä¸‹è¾“å‡ºç»“æœ
    if not args.input:
        print("\nğŸ“‹ é¢„æµ‹ç»“æœ (JSONæ ¼å¼):")
        for result in json_results:
            print(f"æ–‡æœ¬: {result['text']}")
            if result['entities']:
                print("è¯†åˆ«åˆ°çš„å®ä½“:")
                for entity in result['entities']:
                    print(f"  â€¢ {entity['type']}: {entity['text']} (ä½ç½®: {entity['start']}-{entity['end']})")
            else:
                print("æ²¡æœ‰è¯†åˆ«åˆ°å®ä½“")
            print()

elif args.format == 'text':
    # è¾“å‡ºæ–‡æœ¬æ ‡æ³¨æ ¼å¼
    text_results = []
    
    for result in all_results:
        text = result['text']
        entities = sorted(result['entities'], key=lambda x: x['start'])
        
        # ä¸ºé¿å…æ ‡æ³¨ä½ç½®é”™ä¹±ï¼Œå…ˆæŒ‰èµ·å§‹ä½ç½®æ’åº
        entity_markers = []
        for entity in entities:
            entity_markers.append((entity['start'], f"[{entity['type']}:"))
            entity_markers.append((entity['end'], f"]"))
        
        entity_markers.sort(key=lambda x: (x[0], x[1].endswith("]")))
        
        # æ’å…¥æ ‡è®°
        marked_text = ""
        last_pos = 0
        for pos, marker in entity_markers:
            marked_text += text[last_pos:pos] + marker
            last_pos = pos
        
        marked_text += text[last_pos:]
        text_results.append(marked_text)
    
    # ä¿å­˜æ–‡æœ¬ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(text_results))
    
    # äº¤äº’æ¨¡å¼ä¸‹è¾“å‡ºç»“æœ
    if not args.input:
        print("\nğŸ“‹ é¢„æµ‹ç»“æœ (æ–‡æœ¬æ ‡æ³¨æ ¼å¼):")
        for marked_text in text_results:
            print(marked_text)
            print()

elif args.format == 'bio':
    # è¾“å‡ºBIOæ ¼å¼
    bio_results = []
    
    for result in all_results:
        text = result['text']
        entities = result['entities']
        
        # åˆå§‹åŒ–æ‰€æœ‰å­—ç¬¦ä¸º"O"æ ‡ç­¾
        bio_tags = ["O"] * len(text)
        
        # æ’å…¥å®ä½“æ ‡ç­¾
        for entity in entities:
            entity_type = entity['type']
            start = entity['start']
            end = entity['end']
            
            # è®¾ç½®B-æ ‡ç­¾ï¼ˆå®ä½“çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼‰
            if start < len(bio_tags):
                bio_tags[start] = f"B-{entity_type}"
            
            # è®¾ç½®I-æ ‡ç­¾ï¼ˆå®ä½“çš„å…¶ä»–å­—ç¬¦ï¼‰
            for i in range(start + 1, end):
                if i < len(bio_tags):
                    bio_tags[i] = f"I-{entity_type}"
        
        # æ‹¼æ¥å­—ç¬¦å’Œæ ‡ç­¾
        bio_lines = []
        for char, tag in zip(text, bio_tags):
            bio_lines.append(f"{char}\t{tag}")
        
        bio_results.append('\n'.join(bio_lines))
    
    # ä¿å­˜BIOç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n\n'.join(bio_results))
    
    # äº¤äº’æ¨¡å¼ä¸‹è¾“å‡ºç»“æœ
    if not args.input:
        print("\nğŸ“‹ é¢„æµ‹ç»“æœ (BIOæ ¼å¼):")
        for bio_text in bio_results[:3]:  # é™åˆ¶è¾“å‡ºå‰å‡ ä¸ªç»“æœï¼Œé¿å…å¤ªé•¿
            print(bio_text)
            print()
        if len(bio_results) > 3:
            print("... ï¼ˆæ›´å¤šç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶ï¼‰")
    
logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {output_file}")
print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_file}")

# å½“åœ¨äº¤äº’æ¨¡å¼ä¸‹æ—¶ï¼Œæä¾›ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
if not args.input:
    # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
    entity_counts = {}
    for result in all_results:
        for entity in result['entities']:
            entity_type = entity['type']
            if entity_type not in entity_counts:
                entity_counts[entity_type] = 0
            entity_counts[entity_type] += 1
    
    if entity_counts:
        print("\nğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {entity_type}: {count} ä¸ª")
    else:
        print("\nâ— æœªè¯†åˆ«åˆ°ä»»ä½•å®ä½“")