# train_enhanced.py - å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬ï¼Œæ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½

import os
import torch
import logging
import random
import numpy as np
from torch.utils.data import DataLoader
from transformers import BertTokenizerFast, get_scheduler
from model.bert_crf_model import BertCRF
from model.bert_attention_crf_model import BertAttentionCRF
from utils import NERDataset, get_label_list
from data_augmentation import DataAugmentation, create_sample_dictionaries
from model_optimization import ModelOptimizer
from model_explainability import ModelExplainer
import matplotlib.pyplot as plt
from tqdm import tqdm
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report
from config import model_config, augmentation_config, optimization_config, explainability_config
from datetime import datetime
import json

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# æ£€æµ‹GPU
print("âœ… æ˜¯å¦æ£€æµ‹åˆ° GPU:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ–¥ï¸ å½“å‰ GPU åç§°:", torch.cuda.get_device_name(0))
    print("ğŸ”¥ å½“å‰è®¾å¤‡:", torch.device("cuda"))
else:
    print("âŒ å½“å‰ä½¿ç”¨çš„æ˜¯ CPU")

# è®¾ç½®æ—¥å¿—
log_dir = model_config['log_dir']
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, f'train_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(model_config['model_dir'], exist_ok=True)
os.makedirs(explainability_config['visualization_output_dir'], exist_ok=True)

# åŠ è½½åˆ†è¯å™¨
tokenizer = BertTokenizerFast.from_pretrained(model_config['bert_model_name'])

# æ„å»ºæ ‡ç­¾æ˜ å°„
label_list = get_label_list([
    model_config['train_path'],
    model_config['dev_path'],
    model_config['test_path']
])
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}
num_labels = len(label_list)

logger.info(f"æ ‡ç­¾æ€»æ•°: {num_labels}")
logger.info(f"æ ‡ç­¾åˆ—è¡¨: {label_list}")

# æ•°æ®å¢å¼º
if augmentation_config['use_data_augmentation']:
    logger.info("æ­£åœ¨å‡†å¤‡æ•°æ®å¢å¼º...")
    
    # æ£€æŸ¥åŒä¹‰è¯å’Œå®ä½“è¯å…¸æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºç¤ºä¾‹è¯å…¸
    if not os.path.exists(augmentation_config['synonym_dict_path']) or \
       not os.path.exists(augmentation_config['entity_dict_path']):
        logger.info("æœªæ‰¾åˆ°è¯å…¸æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹è¯å…¸...")
        create_sample_dictionaries()
    
    # åˆå§‹åŒ–æ•°æ®å¢å¼ºå™¨
    augmenter = DataAugmentation(
        synonym_dict_path=augmentation_config['synonym_dict_path'],
        entity_dict_path=augmentation_config['entity_dict_path']
    )
    
    # è¯»å–åŸå§‹è®­ç»ƒæ•°æ®
    with open(model_config['train_path'], 'r', encoding='utf-8') as f:
        train_data = f.read().strip().split('\n\n')
    
    # å¢å¼ºæ•°æ®
    augmented_data = []
    for sample in tqdm(train_data, desc="æ•°æ®å¢å¼º"):
        if not sample.strip():
            continue
        
        # è§£ææ ·æœ¬
        lines = sample.strip().split('\n')
        words, labels = [], []
        for line in lines:
            if line.strip():
                parts = line.strip().split()
                if len(parts) == 2:
                    words.append(parts[0])
                    labels.append(parts[1])
        
        # åº”ç”¨æ•°æ®å¢å¼º
        if random.random() < 0.7:  # åªå¯¹70%çš„æ ·æœ¬è¿›è¡Œå¢å¼º
            aug_words, aug_labels = augmenter.augment_data(
                words, labels,
                use_synonym_replace=augmentation_config['use_synonym_replace'],
                use_entity_replace=augmentation_config['use_entity_replace'],
                synonym_replace_ratio=augmentation_config['synonym_replace_ratio'],
                entity_replace_ratio=augmentation_config['entity_replace_ratio']
            )
            
            # æ·»åŠ å¢å¼ºåçš„æ ·æœ¬
            aug_sample = '\n'.join([f"{w}\t{l}" for w, l in zip(aug_words, aug_labels)])
            augmented_data.append(aug_sample)
    
    # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
    all_data = train_data + augmented_data
    logger.info(f"æ•°æ®å¢å¼ºå®Œæˆ: åŸå§‹æ ·æœ¬æ•° {len(train_data)}, å¢å¼ºåæ ·æœ¬æ•° {len(all_data)}")
    
    # ä¿å­˜å¢å¼ºåçš„æ•°æ®
    augmented_train_path = model_config['train_path'].replace('.txt', '_augmented.txt')
    with open(augmented_train_path, 'w', encoding='utf-8') as f:
        f.write('\n\n'.join(all_data))
    
    # æ›´æ–°è®­ç»ƒæ•°æ®è·¯å¾„
    train_path = augmented_train_path
else:
    train_path = model_config['train_path']

# åŠ è½½æ•°æ®é›†
train_dataset = NERDataset(train_path, tokenizer, label2id, model_config['max_len'])
dev_dataset = NERDataset(model_config['dev_path'], tokenizer, label2id, model_config['max_len'])

train_loader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=model_config['eval_batch_size'])

# åˆå§‹åŒ–æ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# é€‰æ‹©æ¨¡å‹ç±»å‹
if model_config['use_self_attention']:
    logger.info("ä½¿ç”¨å¢å¼ºç‰ˆBERT-Attention-CRFæ¨¡å‹")
    model = BertAttentionCRF(model_config['bert_model_name'], num_labels).to(device)
else:
    logger.info("ä½¿ç”¨æ ‡å‡†BERT-CRFæ¨¡å‹")
    model = BertCRF(model_config['bert_model_name'], num_labels).to(device)

# æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
logger.info("æ¨¡å‹ç»“æ„é…ç½®:")
logger.info(f"âœ“ BiLSTMå±‚çŠ¶æ€: {'å¯ç”¨' if model_config['use_bilstm'] else 'ç¦ç”¨'}")
if model_config['use_bilstm']:
    logger.info(f"  - éšè—å±‚å¤§å°: {model_config['lstm_hidden_size']}")
    logger.info(f"  - LSTMå±‚æ•°: {model_config['lstm_layers']}")
    logger.info(f"  - Dropoutç‡: {model_config['lstm_dropout']}")

# åˆå§‹åŒ–æ¨¡å‹ä¼˜åŒ–å™¨
model_optimizer = ModelOptimizer(model, device)

# ä¼˜åŒ–å™¨é…ç½®
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=model_config['learning_rate'],
    weight_decay=model_config['weight_decay']
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦é¢„çƒ­ï¼‰
num_training_steps = len(train_loader) * model_config['num_epochs']
num_warmup_steps = int(num_training_steps * model_config['warmup_ratio'])
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

# éªŒè¯é›†è¯„ä¼°å‡½æ•°
def evaluate_on_dev():
    model.eval()
    true_labels = []
    pred_labels = []
    eval_loss = 0
    num_batches = 0
    start_time = datetime.now()
    
    with torch.no_grad():
        for batch in dev_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            pred = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])
            loss = model.crf.neg_log_likelihood(
                pred,
                batch['labels'],
                batch['attention_mask']
            )
            eval_loss += loss.item()
            num_batches += 1
            
            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            for i in range(batch['input_ids'].size(0)):
                label_ids = batch['labels'][i].tolist()
                pred_ids = pred[i]
                
                tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])
                word_labels = []
                word_preds = []
                
                for token, true_id, pred_id in zip(tokens, label_ids, pred_ids):
                    if token in ["[CLS]", "[SEP]", "[PAD]"] or true_id == -100:
                        continue
                    word_labels.append(id2label[true_id])
                    word_preds.append(id2label[pred_id])
                
                if word_labels:
                    true_labels.append(word_labels)
                    pred_labels.append(word_preds)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    eval_loss = eval_loss / num_batches
    p = precision_score(true_labels, pred_labels)
    r = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)
    
    # è·å–æ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
    report = classification_report(true_labels, pred_labels, output_dict=True)
    
    # è®¡ç®—è¯„ä¼°è€—æ—¶
    eval_time = (datetime.now() - start_time).total_seconds()
    
    return {
        'precision': p,
        'recall': r,
        'f1': f1,
        'loss': eval_loss,
        'report': report,
        'eval_time': eval_time,
        'true_labels': true_labels,
        'pred_labels': pred_labels
    }

# è®­ç»ƒå¾ªç¯

# è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡
train_history = {
    'train_losses': [],
    'eval_losses': [],
    'precisions': [],
    'recalls': [],
    'f1_scores': [],
    'learning_rates': []
}
best_f1 = 0
early_stop_counter = 0
patience = model_config['early_stopping_patience']

for epoch in range(model_config['num_epochs']):
    model.train()
    total_loss = 0
    epoch_start_time = datetime.now()
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    progress_bar = tqdm(train_loader, desc=f"Epoch: {epoch + 1}/{model_config['num_epochs']}")
    
    for step, batch in enumerate(progress_bar):
        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        if model_config['use_amp']:
            loss = model_optimizer.train_step_amp(batch, optimizer)
        else:
            batch = {k: v.to(device) for k, v in batch.items()}
            loss = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'], batch['labels'])
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), model_config['max_grad_norm'])
            
            optimizer.step()
            optimizer.zero_grad()
        
        lr_scheduler.step()
        total_loss += loss
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({'loss': f'{loss:.4f}', 'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'})
        
        # å®šæœŸè®°å½•è®­ç»ƒä¿¡æ¯
        if (step + 1) % model_config['logging_steps'] == 0:
            logger.info(
                f'Epoch: {epoch+1}/{model_config["num_epochs"]}, '
                f'Step: {step+1}/{len(train_loader)}, '
                f'Loss: {loss:.4f}, '
                f'LR: {optimizer.param_groups[0]["lr"]:.2e}'
            )
    
    # åº”ç”¨æ¨¡å‹å‰ªæï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if optimization_config['use_model_pruning'] and epoch == optimization_config['apply_pruning_epoch']:
        logger.info(f"åœ¨ç¬¬ {epoch+1} ä¸ªepochåº”ç”¨æ¨¡å‹å‰ªæ..")
        model_optimizer.apply_pruning(
            amount=optimization_config['pruning_amount'],
            method=optimization_config['pruning_method']
        )
        logger.info("æ¨¡å‹å‰ªæå®Œæˆ")
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_train_loss = total_loss / len(train_loader)
    current_lr = optimizer.param_groups[0]['lr']
    
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    eval_results = evaluate_on_dev()
    
    # æ›´æ–°è®­ç»ƒå†å²
    train_history['train_losses'].append(avg_train_loss)
    train_history['eval_losses'].append(eval_results['loss'])
    train_history['precisions'].append(eval_results['precision'])
    train_history['recalls'].append(eval_results['recall'])
    train_history['f1_scores'].append(eval_results['f1'])
    train_history['learning_rates'].append(current_lr)
    
    # è®¡ç®—è®­ç»ƒè€—æ—¶
    epoch_time = (datetime.now() - epoch_start_time).total_seconds()
    
    # è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
    logger.info(f"\n{'='*50}\nEpoch {epoch + 1} è®­ç»ƒæŠ¥å‘Š\n{'='*50}")
    logger.info(f"è®­ç»ƒæ—¶é—´: {epoch_time:.2f}ç§’")
    logger.info(f"è¯„ä¼°æ—¶é—´: {eval_results['eval_time']:.2f}ç§’")
    logger.info(f"\nè®­ç»ƒæŒ‡æ ‡:")
    logger.info(f"  - è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
    logger.info(f"  - è¯„ä¼°æŸå¤±: {eval_results['loss']:.4f}")
    logger.info(f"  - å­¦ä¹ ç‡: {current_lr:.2e}")
    
    logger.info(f"\næ•´ä½“è¯„ä¼°æŒ‡æ ‡:")
    logger.info(f"  - å‡†ç¡®ç‡: {eval_results['precision']:.4f}")
    logger.info(f"  - å¬å›ç‡: {eval_results['recall']:.4f}")
    logger.info(f"  - F1å€¼: {eval_results['f1']:.4f}")

    # è¾“å‡ºæ¯ä¸ªæ ‡ç­¾çš„è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
    logger.info(f"\nå„æ ‡ç­¾è¯„ä¼°æŒ‡æ ‡:")
    for label in label_list:
        if label == 'O':
            continue  # è·³è¿‡Oæ ‡ç­¾
        if label in eval_results['report']:
            metrics = eval_results['report'][label]
            logger.info(f"\n{label}:")
            logger.info(f"  - æ ·æœ¬æ•°: {metrics['support']}")
            logger.info(f"  - å‡†ç¡®ç‡: {metrics['precision']:.4f}")
            logger.info(f"  - å¬å›ç‡: {metrics['recall']:.4f}")
            logger.info(f"  - F1å€¼: {metrics['f1-score']:.4f}")

    # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡å˜åŒ–å›¾
    if len(train_history['train_losses']) > 0:
        plt.figure(figsize=(12, 8))
        plt.subplot(2, 2, 1)
        plt.plot(train_history['train_losses'], label='è®­ç»ƒæŸå¤±')
        plt.plot(train_history['eval_losses'], label='éªŒè¯æŸå¤±')
        plt.legend()
        plt.title('æŸå¤±æ›²çº¿')

        plt.subplot(2, 2, 2)
        plt.plot(train_history['precisions'], label='å‡†ç¡®ç‡')
        plt.plot(train_history['recalls'], label='å¬å›ç‡')
        plt.plot(train_history['f1_scores'], label='F1å€¼')
        plt.legend()
        plt.title('è¯„ä¼°æŒ‡æ ‡æ›²çº¿')

        plt.subplot(2, 2, 3)
        plt.plot(train_history['learning_rates'], label='å­¦ä¹ ç‡')
        plt.legend()
        plt.title('å­¦ä¹ ç‡å˜åŒ–æ›²çº¿')

        plt.tight_layout()
        plt.savefig(os.path.join(model_config['model_dir'], 'training_metrics.png'))
        plt.close()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if eval_results['f1'] > best_f1:
        best_f1 = eval_results['f1']
        torch.save(model.state_dict(), model_config['best_model_path'])
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹çš„åˆ†ç±»æŠ¥å‘Š
        report_path = os.path.join(model_config['model_dir'], 'best_classification_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"Best Model Classification Report (Epoch {epoch + 1})\n")
            f.write(f"F1 Score: {best_f1:.4f}\n\n")
            f.write(json.dumps(eval_results['report'], indent=2, ensure_ascii=False))
        
        logger.info(f"\nâœ¨ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ï¼ŒF1: {best_f1:.4f}")
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        logger.info(f"\nâš ï¸ æœªæå‡ï¼ŒEarlyStopping è®¡æ•°: {early_stop_counter}/{patience}")
        if early_stop_counter >= patience:
            logger.info("\nğŸ›‘ æå‰åœæ­¢è®­ç»ƒï¼ˆéªŒè¯é›† F1 æ— æå‡ï¼‰")
            break

# å¦‚æœå¯ç”¨äº†æ¨¡å‹å‰ªæï¼Œç§»é™¤å‰ªæ
if optimization_config['use_model_pruning']:
    logger.info("ç§»é™¤å‰ªæï¼Œä½¿æƒé‡æ°¸ä¹…ä¿æŒå‰ªæåçš„å€¼...")
    model_optimizer.remove_pruning()

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
torch.save(model.state_dict(), model_config['final_model_path'])
print(f"\næ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ {model_config['final_model_path']}")

# å¯è§†åŒ–è®­ç»ƒæŸå¤±
plt.figure()
loss_history_cpu = [loss.cpu().item() for loss in loss_history]
plt.plot(range(1, len(loss_history_cpu) + 1), loss_history_cpu, marker='o')
plt.xlabel("Epoch")
plt.ylabel("Average Loss")
plt.title("Training Loss Over Epochs")
plt.grid(True)
plt.savefig(os.path.join(model_config['model_dir'], "training_loss.png"))

# æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ
if any([explainability_config['generate_attention_visualization'],
        explainability_config['generate_token_attention'],
        explainability_config['generate_prediction_confidence'],
        explainability_config['generate_html_visualization']]):
    
    logger.info("å¼€å§‹æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(model_config['best_model_path']))
    model.eval()
    
    # åˆå§‹åŒ–æ¨¡å‹è§£é‡Šå™¨
    explainer = ModelExplainer(model, tokenizer, id2label, device)
    
    # ä»éªŒè¯é›†ä¸­é€‰æ‹©æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    with open(model_config['dev_path'], 'r', encoding='utf-8') as f:
        dev_samples = f.read().strip().split('\n\n')
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    num_samples = min(explainability_config['visualization_samples'], len(dev_samples))
    selected_samples = random.sample(dev_samples, num_samples)
    
    for i, sample in enumerate(selected_samples):
        # è§£ææ ·æœ¬
        lines = sample.strip().split('\n')
        words = [line.split()[0] for line in lines if line.strip()]
        text = ''.join(words)
        
        # åˆ›å»ºæ ·æœ¬ç›®å½•
        sample_dir = os.path.join(explainability_config['visualization_output_dir'], f"sample_{i+1}")
        os.makedirs(sample_dir, exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ–‡æœ¬
        with open(os.path.join(sample_dir, "text.txt"), "w", encoding="utf-8") as f:
            f.write(text)
        
        # ç”Ÿæˆå¯è§†åŒ–
        if explainability_config['generate_attention_visualization']:
            explainer.visualize_attention(text, os.path.join(sample_dir, "attention_heatmap.png"))
        
        if explainability_config['generate_token_attention']:
            explainer.visualize_token_attention(text, os.path.join(sample_dir, "token_attention.png"))
        
        if explainability_config['generate_prediction_confidence']:
            explainer.visualize_prediction_confidence(text, os.path.join(sample_dir, "prediction_confidence.png"))
        
        if explainability_config['generate_html_visualization']:
            explainer.generate_html_visualization(text, os.path.join(sample_dir, "visualization.html"))
    
    logger.info(f"æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ {explainability_config['visualization_output_dir']}")

# æ¨¡å‹é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if optimization_config['use_model_quantization']:
    logger.info("å¼€å§‹æ¨¡å‹é‡åŒ–...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(model_config['best_model_path']))
    model.eval()
    
    try:
        # é‡åŒ–æ¨¡å‹
        quantized_model = model_optimizer.quantize_model(optimization_config['quantization_type'])
        
        # ä¿å­˜é‡åŒ–æ¨¡å‹
        quantized_model_path = os.path.join(model_config['model_dir'], "quantized_model.pth")
        model_optimizer.save_quantized_model(quantized_model_path, quantized_model)
        
        # æ¯”è¾ƒæ¨¡å‹å¤§å°
        original_size, quantized_size = model_optimizer.compare_model_sizes(
            model_config['best_model_path'],
            quantized_model_path
        )
        
        # ä¿å­˜é‡åŒ–ç»“æœ
        quantization_result = {
            "original_size_mb": original_size,
            "quantized_size_mb": quantized_size,
            "reduction_percentage": (1 - quantized_size / original_size) * 100
        }
        
        with open(os.path.join(model_config['model_dir'], "quantization_result.json"), "w") as f:
            json.dump(quantization_result, f, indent=2)
        
        logger.info(f"æ¨¡å‹é‡åŒ–å®Œæˆï¼Œé‡åŒ–æ¨¡å‹å·²ä¿å­˜è‡³ {quantized_model_path}")
    except Exception as e:
        logger.error(f"æ¨¡å‹é‡åŒ–å¤±è´¥: {e}")

print("\nğŸ‰ è®­ç»ƒå’Œä¼˜åŒ–æµç¨‹å…¨éƒ¨å®Œæˆ!")